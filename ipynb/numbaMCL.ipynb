{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the package\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the data\n",
    "np.random.seed(42)\n",
    "start = np.random.randint(1,10,(1024,1024))\n",
    "symmetry = np.tril(start,k=0) + np.tril(start,k=-1).T  # [1024,1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's design the necessary sequential version functions of MCL algorithm\n",
    "def add_self_loop(adjacency,loop_value):\n",
    "    for i in range(adjacency.shape[0]):\n",
    "        adjacency[i,i] += loop_value\n",
    "    return adjacency\n",
    "\n",
    "def l1_normalization_like(adjacency):\n",
    "    result = np.empty(adjacency.shape)\n",
    "    for j in range(adjacency.shape[1]):\n",
    "        result[:,j] = adjacency[:,j] / np.sum(adjacency[:,j])\n",
    "    return result\n",
    "\n",
    "def expand(adjacency,expansion):\n",
    "    result = adjacency ** expansion\n",
    "    return result\n",
    "\n",
    "def inflate(adjacency,inflation):\n",
    "    result = l1_normalization_like(adjacency**2)\n",
    "    return result\n",
    "\n",
    "def iterate(adjacency,expansion=2,inflation=2):\n",
    "    result = inflate(expand(adjacency,expansion),inflation)\n",
    "    return result\n",
    "\n",
    "def allclose(matrix1,matrix2,atol=1e-08,rtol=1e-05):\n",
    "    c = np.abs(matrix1-matrix2) - (atol + rtol*np.abs(matrix2))\n",
    "    if np.max(c) <= 0:\n",
    "        return (True,c)\n",
    "    else:\n",
    "        return (False,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 ms ± 3.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# let's run the sequential MCL\n",
    "adjacency = symmetry\n",
    "\n",
    "# add self-loop (Optional)\n",
    "\n",
    "# l1 norm of each column, distort the symmetry of the original adjacency matrix\n",
    "adjacency = l1_normalization_like(adjacency)\n",
    "\n",
    "for i in range(1000000000):\n",
    "    # iterate (expand + inflate)\n",
    "    adjacency_old = np.copy(adjacency)\n",
    "    adjacency_new = iterate(adjacency)\n",
    "    # check convergence\n",
    "    isConverged,c_matrix = allclose(adjacency_old,adjacency_new)\n",
    "    if isConverged:\n",
    "        break\n",
    "    else:\n",
    "        adjacency = adjacency_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's dive into the cuda version, the parallel version of MCL\n",
    "# implement outline:\n",
    "# 1. write three kernel function: expand, inflate, check_converge\n",
    "# 2. run the MCL from the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel function1 : expand\n",
    "@cuda.jit\n",
    "def gpu_expand(in_matrix,out_matrix):  # 2-d block,threads\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    blocksize_x = cuda.blockDim.x\n",
    "    blocksize_y = cuda.blockDim.y\n",
    "    pos_x = tx + blocksize_x * bx\n",
    "    pos_y = ty + blocksize_y * by\n",
    "    if pos_x <= in_matrix.shape[0] and pos_y <= in_matrix.shape[1]:\n",
    "        out_matrix[pos_x,pos_y] = in_matrix[pos_x,pos_y] ** 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel function2: inflation\n",
    "@cuda.jit\n",
    "def gpu_inflate(in_matrix,out_matrix,final_matrix):  # 1-d block, threads\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    blocksize_x = cuda.blockDim.x\n",
    "    pos_x =  tx + blocksize_x * bx\n",
    "    if pos_x <= in_matrix.shape[1]:\n",
    "        for i in range(out_matrix.shape[0]):\n",
    "            out_matrix[i,pos_x] = in_matrix[i,pos_x] ** 2\n",
    "        sum_ = 0\n",
    "        for i in range(out_matrix.shape[0]):\n",
    "            sum_ += out_matrix[:,pos_x][i]\n",
    "        for i in range(out_matrix.shape[0]):\n",
    "            final_matrix[i,pos_x] = out_matrix[i,pos_x] / sum_  # within-column normalization\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel function3: check_converge\n",
    "@cuda.jit\n",
    "def gpu_check_converge(old_matrix,new_matrix,result_matrix):\n",
    "    atol = 1e-5\n",
    "    rtol = 1e-8\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    blocksize_x = cuda.blockDim.x\n",
    "    blocksize_y = cuda.blockDim.y\n",
    "    pos_x = tx + blocksize_x * bx\n",
    "    pos_y = ty + blocksize_y * by\n",
    "    if pos_x <= old_matrix.shape[0] and pos_y <= old_matrix.shape[1]:\n",
    "        c = abs(new_matrix[pos_x,pos_y] - old_matrix[pos_x,pos_y]) - (atol + rtol*abs(old_matrix[pos_x,pos_y]))\n",
    "        if c <= 0: \n",
    "            result_matrix[pos_x,pos_y] = True\n",
    "        else:\n",
    "            result_matrix[pos_x,pos_y] = False\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 ms ± 2.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# let's run the gpu version of MCL\n",
    "\n",
    "adjacency = symmetry\n",
    "adjacency = l1_normalization_like(adjacency)\n",
    "\n",
    "blocks_per_grid = (32,32)\n",
    "threads_per_block = (32,32)\n",
    "\n",
    "for n in range(4):\n",
    "    # expand\n",
    "    out_matrix = np.empty([1024,1024])\n",
    "    d_adjacency = cuda.to_device(adjacency)\n",
    "    d_out_matrix = cuda.to_device(out_matrix)\n",
    "\n",
    "    gpu_expand[blocks_per_grid,threads_per_block](d_adjacency,d_out_matrix)\n",
    "    d_out_matrix.copy_to_host(out_matrix)\n",
    "\n",
    "    # inflate\n",
    "    out_matrix1 = np.empty([1024,1024])\n",
    "    final_matrix = np.empty([1024,1024])\n",
    "    d_out_matrix = cuda.to_device(out_matrix)\n",
    "    d_out_matrix1 = cuda.to_device(out_matrix1)\n",
    "    d_final_matrix = cuda.to_device(final_matrix)\n",
    "    gpu_inflate[blocks_per_grid,threads_per_block](d_out_matrix,d_out_matrix1,d_final_matrix)\n",
    "    d_final_matrix.copy_to_host(final_matrix)\n",
    "\n",
    "    # check convergence\n",
    "    d_adjacency = cuda.to_device(adjacency)\n",
    "    d_final_matrix = cuda.to_device(final_matrix)\n",
    "    result_matrix = np.empty([1024,1024])\n",
    "    d_result_matrix = cuda.to_device(result_matrix)\n",
    "    d_final_matrix = cuda.to_device(final_matrix)\n",
    "    gpu_check_converge[blocks_per_grid,threads_per_block](d_adjacency,d_final_matrix,d_result_matrix)\n",
    "    d_result_matrix.copy_to_host(result_matrix)\n",
    "    cond = np.all(result_matrix)\n",
    "    if cond == True:\n",
    "        break\n",
    "    else:\n",
    "        adjacency = result_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 [python/3.6 cuda/10.2.89]",
   "language": "python",
   "name": "sys_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
